{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd6iFKF2gohh"
   },
   "source": [
    "# **CDS Project: Part 1**\n",
    "\n",
    "*Institute of Software Security (E22)*  \n",
    "*Hamburg University of Technology*  \n",
    "*SoSe 2023*\n",
    "\n",
    "## Learning objectives\n",
    "---\n",
    "\n",
    "- Use a basic Machine Learning (ML) pipeline with pre-trained models.\n",
    "- Build your own data loader.\n",
    "- Load and run a pre-trained ML model.\n",
    "- Evaluate the performance of an ML model.\n",
    "- Calculate and interpret performance metrics.\n",
    "\n",
    "## Materials\n",
    "---\n",
    "\n",
    "- Lecture Slides 1, 2, and 3.\n",
    "- PyTorch Documentation: [Datasets and Data Loaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybWt0W4gjbiC"
   },
   "source": [
    "## Project Description\n",
    "---\n",
    "\n",
    "In this project, you are given an ML model that is pre-trained on a vulnerability dataset. The dataset consists of code samples labeled with True or False flags, depending on the presence and absense of a vulnerability. Your goal is to use the pre-trained model to predict if the code samples in the validation set contain vulnerabilities or not and analyse the results. Please proceed to the below tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrciLvqNj96k"
   },
   "source": [
    "###*Task 1*\n",
    "\n",
    "Build a data loader for the validation dataset present in the following path: \"*data_students/student_dataset.hdf5*\". You will be using this dataset to validate the performance of the ML model. The dataset is in HDF5 binary data format. This format is used to store large amount of data. Make sure that you import and familiarise yourself with the right Python libraries to handle HDF5 files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Eem6AZNyyXsn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the HDF5 file: ['labels', 'source', 'vectors']\n"
     ]
    }
   ],
   "source": [
    "# TODO: import the necessary libraries to load the data from the specified path.\n",
    "import h5py\n",
    "\n",
    "# Load the dataset\n",
    "with h5py.File('data_students/student_dataset.hdf5', 'r') as f:\n",
    "    print(\"Keys in the HDF5 file:\", list(f.keys()))\n",
    "  \n",
    "    X = f['labels'][:]  \n",
    "    y = f['vectors'][:]  \n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create dataset and loader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARwcBrbFlMu8"
   },
   "source": [
    "###*Task 2*\n",
    "\n",
    "Generate a table with 10 random samples from the dataset and show their corresponding labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AuYminA_mTnJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample # Label Shape        First 3 Label Values\n",
      "        1    (1, 768)      [0.4667, 1.1838, 0.32]\n",
      "        2    (1, 768)    [1.4171, 0.5902, -0.012]\n",
      "        3    (1, 768) [-0.9566, -2.0477, -3.4263]\n",
      "        4    (1, 768)  [2.2081, -0.6864, -2.8236]\n",
      "        5    (1, 768) [-0.3244, -0.0013, -1.4802]\n",
      "        6    (1, 768)   [0.7624, 0.1513, -1.3426]\n",
      "        7    (1, 768)  [2.5526, -0.5399, -0.4223]\n",
      "        8    (1, 768)    [0.5805, 0.1009, 0.1584]\n",
      "        9    (1, 768)   [-0.3426, 0.6134, 0.2494]\n",
      "       10    (1, 768)    [1.0751, 3.053, -3.9202]\n"
     ]
    }
   ],
   "source": [
    "# TODO: display 10 random samples from the loaded dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Get 10 random samples\n",
    "indices = random.sample(range(len(dataset)), 10)\n",
    "samples = [dataset[i] for i in indices]\n",
    "\n",
    "# Create a summary table\n",
    "data = []\n",
    "for i, (features, label) in enumerate(samples, 1):\n",
    "    sample_info = {\n",
    "        'Sample #': i,\n",
    "       \n",
    "        'Label Shape': tuple(label.shape),\n",
    "        'First 3 Label Values': [round(x.item(), 4) for x in label.flatten()[:3]]\n",
    "    }\n",
    "    data.append(sample_info)\n",
    "\n",
    "# Create and display pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da5YCWVkmUL2"
   },
   "source": [
    "###*Task 3*\n",
    "\n",
    "Inspect the dataset and answer the following questions:\n",
    "1.  How many samples are in the dataset?\n",
    "2. How many positive examples (vulnerability-labeled instances) are in the dataset?\n",
    "3. What is the vulnerable/non-vulnerable ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LDpozMCfnnJg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Total samples: 1000\n"
     ]
    }
   ],
   "source": [
    "# TODO: inspect and understand the loaded dataset\n",
    "\n",
    "total_samples = len(dataset)\n",
    "print(f\"1. Total samples: {stats['total_samples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vulnerable samples: 283\n"
     ]
    }
   ],
   "source": [
    "# 2. How many positive examples (vulnerability-labeled instances) are in the dataset?\n",
    "\n",
    "vulnerable_count = (X_tensor == 1).sum().item()\n",
    "print(f\"Number of vulnerable samples: {vulnerable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vulnerable/Non-vulnerable ratio: 0.39:1\n",
      "(283 vulnerable vs 717 non-vulnerable)\n"
     ]
    }
   ],
   "source": [
    "#3. What is the vulnerable/non-vulnerable ratio?\n",
    "vulnerable = (X_tensor == 1).sum().item()\n",
    "non_vulnerable = (X_tensor == 0).sum().item()\n",
    "\n",
    "ratio = vulnerable / non_vulnerable if non_vulnerable > 0 else float('inf')\n",
    "\n",
    "print(f\"Vulnerable/Non-vulnerable ratio: {ratio:.2f}:1\")\n",
    "print(f\"({vulnerable} vulnerable vs {non_vulnerable} non-vulnerable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UivWlO3dnngr"
   },
   "source": [
    "###*Task 4*\n",
    "\n",
    "Load and run the following pre-trained neural network model called VulnPredictionModel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Jex8XdkFJhb"
   },
   "source": [
    "``` python \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Model architecture:\n",
      "VulnPredictModel(\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "# 1. Define the model with proper forward pass\n",
    "class VulnPredictModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(768, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, 768]\n",
    "        return self.linear_stack(x)\n",
    "\n",
    "# 2. Initialize model and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VulnPredictModel().to(device)\n",
    "      \n",
    "\n",
    "# TODO: intialize and load the model\n",
    "model=VulnPredictModel()\n",
    "model.load_state_dict(torch.load('model_2023-03-28_20-03.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # Verify model structure\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A9M9ID0n2Gx"
   },
   "source": [
    "###*Task 5*\n",
    "\n",
    "Make a prediction on the provided dataset and compute the following values:\n",
    "- True Positives\n",
    "- True Negatives\n",
    "- False Positives\n",
    "- False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X shape: (1000,)\n",
      "Original y shape: (1000, 1, 768)\n",
      "Corrected features shape: (1000, 768)\n",
      "Corrected labels shape: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original X shape:\", X.shape)  \n",
    "print(\"Original y shape:\", y.shape)  \n",
    "\n",
    "\n",
    "features = y.squeeze()  \n",
    "labels = X.reshape(-1, 1) \n",
    "\n",
    "\n",
    "print(\"Corrected features shape:\", features.shape)  \n",
    "print(\"Corrected labels shape:\", labels.shape)      \n",
    "\n",
    "#DataLoader\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in dataloader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_features)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            \n",
    "            TP += ((preds == 1) & (batch_labels == 1)).sum().item()\n",
    "            FP += ((preds == 1) & (batch_labels == 0)).sum().item()\n",
    "            TN += ((preds == 0) & (batch_labels == 0)).sum().item()\n",
    "            FN += ((preds == 0) & (batch_labels == 1)).sum().item()\n",
    "    \n",
    "    return TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "R8KdeQ2Rn-2Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "- True Positives (TP): 20\n",
      "- False Positives (FP): 1\n",
      "- True Negatives (TN): 716\n",
      "- False Negatives (FN): 263\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: makethe prediction for all the samples in the validation set.\n",
    "TP, FP, TN, FN = evaluate(model, dataloader)\n",
    "\n",
    "print(f\"\"\"\n",
    "Evaluation Metrics:\n",
    "- True Positives (TP): {TP}\n",
    "- False Positives (FP): {FP}\n",
    "- True Negatives (TN): {TN}\n",
    "- False Negatives (FN): {FN}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaFHwiVwow7s"
   },
   "source": [
    "### *Task 6*\n",
    "\n",
    "Compute the corresponding performance metrics **manually** (do not use PyTorch's predefined metrics):\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "KE2daH3LpGEc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manual Performance Metrics:\n",
      "───────────────────────────────────\n",
      "1. Accuracy  = (TP+TN)/Total = (20+716)/(1000) = 0.7360\n",
      "\n",
      "\n",
      "2. Precision = TP/(TP+FP) = 20/(20+1) = 0.9524\n",
      "\n",
      "\n",
      "3. Recall    = TP/(TP+FN) = 20/(20+263) = 0.0707\n",
      "\n",
      "\n",
      "4. F1-Score  = 2*(Precision*Recall)/(Precision+Recall) = 0.1316\n",
      "\n",
      "───────────────────────────────────\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate accuracy\n",
    "\n",
    "\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: calculate precision\n",
    " \n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "# TODO: calculate recall\n",
    "\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "\n",
    "# TODO: calculate F1-score\n",
    "\n",
    "\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\"\"\n",
    "Manual Performance Metrics:\n",
    "───────────────────────────────────\n",
    "1. Accuracy  = (TP+TN)/Total = ({TP}+{TN})/({TP+FP+TN+FN}) = {accuracy:.4f}\n",
    "  \n",
    "\n",
    "2. Precision = TP/(TP+FP) = {TP}/({TP}+{FP}) = {precision:.4f}\n",
    "   \n",
    "\n",
    "3. Recall    = TP/(TP+FN) = {TP}/({TP}+{FN}) = {recall:.4f}\n",
    "  \n",
    "\n",
    "4. F1-Score  = 2*(Precision*Recall)/(Precision+Recall) = {f1:.4f}\n",
    "  \n",
    "───────────────────────────────────\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdIkKUPlpGjv"
   },
   "source": [
    "### *Task 7*\n",
    "\n",
    "Based on your performance metrics, answer the following questions:\n",
    "\n",
    "- Explain the impact of accuracy vs. F1 score.\n",
    "\n",
    "Accuracy measures how often the model is correct overall. It’s a general metric, but it can be misleading in imbalanced datasets.\n",
    "\n",
    "In our  case, even though the model has a high accuracy (73.6%), this is mostly due to correctly predicting the many negative (non-vulnerable) samples. But that doesn't mean it's doing well on the important class — the vulnerable code.\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall, and it focuses only on the positive class performance (vulnerable code). It's more reliable when:\n",
    "we care about , how well you're catching a specific class,\n",
    "\n",
    "The dataset is imbalanced, which is true here.\n",
    "\n",
    "In our case, the F1 score is very low (13.16%), which reveals that the model is missing most of the actual vulnerable code, even though it’s accurate overall.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- In this particular problem, which metric one should focus more on?\n",
    "\n",
    "We should focus more on Recall and F1 Score, not Accuracy.\n",
    "\n",
    "Why?\n",
    "\n",
    "In vulnerability prediction:\n",
    "\n",
    "False negatives (missed vulnerabilities) are dangerous — they could lead to undetected security risks.\n",
    "\n",
    "High precision is nice (we’re rarely wrong when we say something is vulnerable), but our recall is extremely low — we’re catching only 7% of real issues!\n",
    "\n",
    "So, if the model is accurate but blind to most actual vulnerabilities, it’s not useful for practical security scanning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Is there a better metric suitable for the use case of vulnerability prediction? Why?\n",
    "\n",
    "Yes — recall, F1 score, and Precision-Recall AUC are better suited for this use case. They emphasize the model’s ability to detect actual vulnerabilities, which is more critical than overall accuracy in a high-risk, imbalanced problem like security vulnerability prediction.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
